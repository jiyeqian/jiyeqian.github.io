<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="author" content="Jiye Qian" />
    <title>机器学习: 正则化</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <link href="/feed/" rel="alternate" title="Jiye Qian" type="application/atom+xml" />
    <link rel="stylesheet" href="/assets/css/style.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default_inline.css" />
    <link rel="stylesheet" href="/assets/css/coderay.css" />

    <script type="text/javascript" src="/assets/js/jquery-1.7.1.min.js"></script>
    <script type="text/javascript" src="/assets/js/outliner.js"></script>

    <!-- MathJax for LaTeX -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        "HTML-CSS": { extensions: ["handle-floats.js"] },
        TeX: { equationNumbers: { autoNumber: "AMS" } },
        tex2jax: {
            inlineMath: [['$$$', '$$$'], ['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        }
    });
    </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F0b514f17fd99b9fb4be74c94bdd2b7db' type='text/javascript'%3E%3C/script%3E"));
</script>

  </head>
<!--  <body>
-->
  <script type="text/javascript">
    function setTimeSpan(){
    	var date = new Date();
    	timeSpan.innerText=date.format('yyyy-MM-dd hh:mm:ss');
    }

    Date.prototype.format = function(format)
		{
    var o =
    	{
    	    "M+" : this.getMonth()+1, //month
    	    "d+" : this.getDate(),    //day
    	    "h+" : this.getHours(),   //hour
    	    "m+" : this.getMinutes(), //minute
    	    "s+" : this.getSeconds(), //second
    	    "q+" : Math.floor((this.getMonth()+3)/3),  //quarter
    	    "S" : this.getMilliseconds() //millisecond
    	}
    	if(/(y+)/.test(format))
    	format=format.replace(RegExp.$1,(this.getFullYear()+"").substr(4 - RegExp.$1.length));
    	for(var k in o)
    	if(new RegExp("("+ k +")").test(format))
    	format = format.replace(RegExp.$1,RegExp.$1.length==1 ? o[k] : ("00"+ o[k]).substr((""+ o[k]).length));
    	return format;
		}
  </script>
  <body onLoad="setInterval(setTimeSpan,1000);">
    <div id="container">
      <div id="main" role="main">
        <header>
        <h1>机器学习: 正则化</h1>
        </header>
        <nav id="real_nav">
        
          <span><a title="Home" href="/">Home</a></span>
        
          <span><a title="Categories" href="/categories/">Categories</a></span>
        
          <span><a title="Tags" href="/tags/">Tags</a></span>
        
          <span><a title="Logs" href="/logs/">Logs</a></span>
        
          <span><a title="About" href="/about/">About</a></span>
        
          <span><a title="Subscribe" href="/feed/">Subscribe</a></span>
        
        </nav>
        <article class="content">
        <section class="meta">
<span class="time">
  <time datetime="2014-10-20">2014-10-20</time>
</span>

 |
<span class="categories">
  categories
  
  <a href="/categories/#研究学术" title="研究学术">研究学术</a>&nbsp;
  
</span>


 |
<span class="tags">
  tags
  
  <a href="/tags/#机器学习" title="机器学习">机器学习</a>&nbsp;
  
  <a href="/tags/#正则化" title="正则化">正则化</a>&nbsp;
  
</span>

</section>
<section class="post">
<h2 id="section">为什么需要正则化</h2>

<p>过拟合（overfitting）是指模型可以在训练集上表现出色，在新数据上性能却很差。解决过拟合问题的方法：</p>

<ul>
  <li>减少特征数目：手工选择特征、利用模型选择；</li>
  <li>正则化（regularization）：保留所有特征，但是减小$\theta_j$，使特征对预测$y$贡献小。</li>
</ul>

<p>正则化通过在代价函数中加入正则化项限制模型参数，避免过拟合。</p>

<p>线性回归和Logistic回归的代价函数追加的正则化项是$\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2$。</p>

<ul>
  <li>$\lambda$称为正则化参数，增大$\lambda$以减小过拟合；</li>
  <li>$\lambda$过大（$10^{10}$）时，有$\theta_j\approx 0~~(j = 1,2,\ldots,n)$，则$h_\theta(x) = \theta_0$，这会导致欠拟合（underfitting）；</li>
  <li>正则化只作用于$j\ge 1$的非常数项，实际上，正则化常数项对结果影响也不大。</li>
</ul>

<p>资料来源：<a href="#ng_ml_r_2014">[1]</a>。</p>

<p>bias &amp; variance:</p>

<ul>
  <li>high bias: 欠拟合；</li>
  <li>high variance: 过拟合。</li>
</ul>

<h2 id="section-1">正则化线性回归</h2>

<h3 id="section-2">代价函数</h3>

<p>\begin{equation}
J(\theta) = \frac{1}{2m}\left( \sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)^2 + \lambda\sum_{j=1}^n\theta_j^2 \right)
\label{eq:cf-linear-regression-r}
\end{equation}</p>

<h3 id="section-3">梯度下降法估计参数</h3>

<p>repeat until convergence {
\begin{equation*}
\begin{aligned}
\theta_0 &amp; := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_0^{(i)} \\ <br />
\theta_j &amp; := \theta_j - \alpha\frac{1}{m}\left(\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_j^{(i)} + \lambda\theta_j\right)~~(j = 1, 2, \ldots, n)
\end{aligned}
\end{equation*}
}</p>

<p>迭代过程可以化为如下形式：
\begin{equation*}
\theta_j := \theta_j\left(1 - \alpha\frac{\lambda}{m} \right) - \alpha\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_j^{(i)};~~(j = 1, 2, \ldots, n)
\end{equation*}</p>

<p>通常$1 - \alpha\frac{\lambda}{m} &lt; 1$，与非正则化的梯度下降法比较，$\theta_j$减小更快。</p>

<h3 id="section-4">正规方程估计参数</h3>

<p>\begin{equation}
\theta = \left(X^TX + \lambda
\begin{bmatrix}
0  &amp;   &amp;        &amp; \\
   &amp; 1 &amp;        &amp; \\
   &amp;   &amp; \ddots &amp; \\
   &amp;   &amp;        &amp; 1
\end{bmatrix}
\right)^{-1}X^Ty
\end{equation}</p>

<p>可以证明，加入正则化项后矩阵始终可逆。</p>

<h2 id="logistic">正则化Logistic回归</h2>

<h3 id="section-5">代价函数</h3>

<p>\begin{equation}
\begin{aligned}
J(\theta)  = &amp;-\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right)\log \left(1-h_\theta\left(x^{(i)}\right)\right)\right) \\
&amp; + \frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2
\end{aligned}
\label{eq:cf-logistic-regression-r}
\end{equation}</p>

<h3 id="section-6">梯度下降法估计参数</h3>

<p>repeat until convergence {
\begin{equation*}
\begin{aligned}
\theta_0 &amp; := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_0^{(i)} \\ <br />
\theta_j &amp; := \theta_j - \alpha\frac{1}{m}\left(\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_j^{(i)} + \lambda\theta_j\right)~~(j = 1, 2, \ldots, n)
\end{aligned}
\end{equation*}
}</p>

<h3 id="matlab">Matlab实现</h3>

<p>第一步：实现Logistic函数</p>

<div class="highlight"><pre><code class="language-matlab"><span class="k">function</span><span class="w"> </span>g <span class="p">=</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span>z<span class="p">)</span><span class="w"></span>
<span class="n">g</span> <span class="p">=</span> <span class="mf">1.0</span> <span class="o">./</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="nb">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">));</span>
<span class="k">end</span></code></pre></div>

<p>第二步：实现代价函数（包含梯度计算）</p>

<div class="highlight"><pre><code class="language-matlab"><span class="k">function</span><span class="w"> </span>[J, grad] <span class="p">=</span><span class="w"> </span><span class="nf">costFunctionReg</span><span class="p">(</span>theta, X, y, lambda<span class="p">)</span><span class="w"></span>
<span class="n">m</span> <span class="p">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span> <span class="c">% number of training examples</span>

<span class="n">h</span> <span class="p">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">);</span>
<span class="n">J</span> <span class="p">=</span> <span class="o">-</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span> <span class="o">.*</span> <span class="nb">log</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">.*</span> <span class="nb">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">))</span> <span class="o">+</span> <span class="c">...</span>
    <span class="n">lambda</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">sum</span><span class="p">(</span><span class="n">theta</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">.^</span> <span class="mi">2</span><span class="p">);</span>
<span class="n">grad</span> <span class="p">=</span> <span class="p">(</span><span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="p">(</span><span class="n">h</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">lambda</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">;</span> <span class="n">theta</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)])</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>

<span class="k">end</span></code></pre></div>

<p>第三步：估计参数</p>

<div class="highlight"><pre><code class="language-matlab"><span class="n">initial_theta</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">lambda</span> <span class="p">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="n">options</span> <span class="p">=</span> <span class="n">optimset</span><span class="p">(</span><span class="s">&#39;GradObj&#39;</span><span class="p">,</span> <span class="s">&#39;on&#39;</span><span class="p">,</span> <span class="s">&#39;MaxIter&#39;</span><span class="p">,</span> <span class="mi">400</span><span class="p">);</span>
<span class="p">[</span><span class="n">theta</span><span class="p">,</span> <span class="n">J</span><span class="p">,</span> <span class="n">exit_flag</span><span class="p">]</span> <span class="p">=</span> <span class="c">...</span>
	<span class="n">fminunc</span><span class="p">(@(</span><span class="n">t</span><span class="p">)(</span><span class="n">costFunctionReg</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lambda</span><span class="p">)),</span> <span class="n">initial_theta</span><span class="p">,</span> <span class="n">options</span><span class="p">);</span></code></pre></div>

<h2 id="section-7">思考问题</h2>

<ol>
  <li>如何推导正则化线性回归的正规方程解？</li>
  <li>求解正则化线性回归的正规方程时，为何矩阵始终可逆？</li>
  <li>如何选取合适的$\lambda$？</li>
</ol>

<h2 id="section-8">参考资料</h2>

<ol class="bibliography"><li><span id="ng_ml_r_2014">[1]A. Ng, “Regularization: The problem of overfitting.” Coursera, 2014 [Online]. Available at: https://class.coursera.org/ml-007</span>
</li></ol>

</section>
<section align="right">
<br/>
<span>
  <a  href="/2014/10/machine-learning-multiple-classification" class="pageNav"  >上一篇</a>
  &nbsp;&nbsp;&nbsp;
  <a  href="/2014/10/machine-learning-perceptron-learning-algorithm" class="pageNav"  >下一篇</a>
</span>
</section>

	
	<ul class="ds-recent-visitors"></ul>
	<div class="ds-thread" data-thread-key="/2014/10/machine-learning-regularization" data-url="http://qianjiye.de/2014/10/machine-learning-regularization" data-title="机器学习: 正则化">
	</div>
	<script type="text/javascript">
	var first_image = document.getElementsByClassName("post")[0].getElementsByTagName("img")[0]; 
	if (first_image != undefined) {
	document.getElementsByClassName("ds-thread")[0].setAttribute("data-image", first_image.src);
	}
	</script>
		
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"jiyeqian"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>


<!-- <script type="text/javascript"> -->
<!-- $(function(){ -->
<!--   $(document).keydown(function(e) { -->
<!--     var url = false; -->
<!--         if (e.which == 37 || e.which == 72) {  // Left arrow and H -->
<!--          -->
<!--         url = '/2014/10/machine-learning-multiple-classification'; -->
<!--          -->
<!--         } -->
<!--         else if (e.which == 39 || e.which == 76) {  // Right arrow and L -->
<!--          -->
<!--         <1!-- url = 'http://qianjiye.de/2014/10/machine-learning-perceptron-learning-algorithm'; --1> -->
<!--         url = '/2014/10/machine-learning-perceptron-learning-algorithm'; -->
<!--          -->
<!--         } else if (e.which == 75) {  // K -->
<!--           url = '#'; -->
<!--         } else if (e.which == 74) { // J -->
<!--         url = '/2014/10/machine-learning-regularization/#timeSpan'; -->
<!--         } -->
<!--         if (url) { -->
<!--             window.location = url; -->
<!--         } -->
<!--   }); -->
<!-- }) -->
<!-- </script> -->

        </article>
      </div>

    <footer>
        <p><small>
            Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> | Copyright 2014 - 2014 by <a href="/about/">Jiye Qian</a> | <span class="label label-info" id="timeSpan"></span></small></p>
    </footer>

    </div>
  </body>
</html>
